---

layout: page
title: Results
permalink: /results/
---

In this section we discuss the performance of the different attacks. First we briefly look at the properties of the clean data. Then we attack the model with different methods and analyze the success. We compare the perceptibility of adversaries generated by the different methods. Finally, we summarize our findings and point to future investigations.

## 1. Data Exploration
Considering that the goal of adversarial examples is to fool the network into predicting a wrong class with high confidence, a good place to start start is to inspect the model's performance without any adversarial perturbation.

In the data there are 452 out of 1000 distinct classes represented. The most frequent class is `ballplayer, baseball player` (class index 981) with 8 occurrences, followed by `racer, race car, racing car`, `stone wall` and `worm fence, snake fence, snake-rail fence, ...` with 7 each. Within these frequent classes, the model's confidence is around $$58$$% with a standard deviation of around $$27$$. This large range in confidence is likely due to false predictions. Figure 1 shows how the the top 5 confidence is distributed for different confidences in their top predictions.

{% include image.html file="Confidence_Distributions.png" description="Figure 1: Top 5 confidences from 100% to 60% in 5% steps." %}

The average confidence for a class is between $$99.99$$% (`barrel, cask`) and $$41.64$$% (`sandal`). The distribution of the average confidence is shown in figure 2. For over half of the examples, the model has an average confidence of over $$60$$% and over $$\frac{2}{3}$$ of the examples have a confidence of over $$50$$%.

{% include image.html file="Data_Exploration-Average-confidence_per_class.png" description="Figure 2: Average confidence per class in descending order. The majority of classes consist of only one sample. The highest number of samples per class is 8." %}


The table shows the model's overall performance.


|                | Confidence    |  Accuracy     |
| :------------- | :----------:  | :----------:  | 
|  Top 1         | 0.69          | 0.84          |
|  Top 5         | 0.63          | 0.97          |

Top 1 means that the predicted class is the correct class. Top 5 means that the correct class is among the 5 predicted classes with the highest score.


## 2. Attacks
We now attack the model with the methods described in the implementation section and analyze their success. Along the way we develop and test hypothesis which apply to adversarial attacks.


### 2.1 Fast Gradient Sign Method
The following is an example of the original image, the generated perturbance, and the resulting adversarial image using the Fast Gradient Sign Method:

{% include image.html file="Sample_766_pair.png" description="Figure 3: Original image, the with the FGSM generated perturbance and resulting adversarial image. This attack decreases the network’s confidence from almost 100% down to 14%." %}

The adversarial image appears slightly blurrier than the original one, like for example taken at poorer resolution or with a worse camera. Without the reference image however, it can be difficult to tell that it has been modified. With increasing attack strength, this becomes noticeable as can be seen in the following images:

{% include image.html file="Sample_766_series.png" description="Figure 4: Original image and a series of adversarial images with increasing attack intensity. The values for epsilon are: 0, 4/255, 8/255, 12/255 and 16/255. Subjectively, at around 12/255 the attack becomes noticeable." %}

The image appears more and more noisy. We later show methods which produce cleaner looking adversaries.

But how effective is this attack in tricking the network? Recall that we want to produce images which cause a prediction of a wrong class at a high confidence. From figure 7 you can see that the confidence drops sharply while the class changes only for a few epsilons. The example above is the sample with which the model has one of the highest confidences on clean data. We have seen this same behaviour with other images with high confidence in the dataset. This leads us to the first hypothesis:

**Hypothesis 1:** Images with a high initial confidence are harder to manipulate.


#### All Images
We consider a plot of accuracy and confidence over the attack strength epsilon. For this hypothesis to be true we would observe a sharp drop in accuracy with increasing attack strength. The higher the initial confidence is, the smaller the slope of the accuracy should be. It is harder to attack the network at the same epsilon. At the same time the confidence should drop slightly since more and more robust features are altered.

Recall from the section Data Exploration how the confidence over all data is distributed. We consider correct initial classifications only and split the data by confidences in ranges of 5% points.

{% include image.html file="FGSM-accuracies_confidences.png" description="Figure 5: Accuracy (left) and confidence (right) for different initial confidences over increasing attack intensity. Examples that are labelled incorrectly by the model without adversarial perturbations are excluded." %}


#### Individual Images
In theory, increasing the size of the perturbation should lower the confidence of the model in predicting the correct class. We, however, have found some exceptions. Figure 6 shows two samples where the FGSM is not able to change the class while the model maintains a relatively high confidence. Interestingly, the confidence in the original correct class first quickly drops then increases again with increasing adversarial perturbance. In the first example, the model's confidence even reaches back to clean levels at the highest levels of perturbance.

{% include image.html file="FGSM-individual_images-same_class.png" description="Figure 6: Two examples for class-invariance under FGSM. On the left is the clean example. Next to it the confidence and if class is correct or not over multiple epsilons. The third plot shows the top 5 confidence for the clean case, whereas the rightmost plot shows the top 5 confidence when the top class is at its lowest confidence. The confidence increases again after an initial dip." %}

A smaller increase in confidence after a dip can also be seen in figure 7. Despite the high initial confidence the adversary is able to change the class here. However, with a stronger attack the model predicts the correct class again. We found this behaviour of a bounce back to the correct class frequently for high initial confidences. Around the epsilon where the class changes we found that the highest and second highest confidences are very similar, whereas the gap to the third highest remains. The decrease in prediction confidence causes the second highest confidence to grow over proportionally which leads to the swap at the lowest point.

{% include image.html file="FGSM-individual_images-bounce_back.png" description="Figure 7: Two examples where the adversary is able to change the class. However, for further increase in attack intensity the model recovers and predicts the correct class while being increasingly confident again. The rightmost plot shows the top 5 confidence for the smallest epsilon where a false class is predicted."%}

The previous examples showed images with high initial confidence. It is not possible to generate an adversary with higher confidence than the original one for these cases. Figure 8 shows where it is possible. The initial confidence is quite low (around 50%). The FGSM can manipulate this image easily and achieves a confidence of 80%!

{% include image.html file="FGSM-individual_images-sample_258.png" description="Figure 8: Example for low initial confidence and greater adversarial confidence. Note how a small perturbation achieves the best results here. In this particular attack the perturbation is so slight that it is not representable in an 8bit image."%}

After analyzing the FGSM we now turn to a method derived from it, the basic iterative method BIM.

### 2.2 Basic Iterative Method
The authors in [Adversarial Examples in the Physical World](http://arxiv.org/abs/1607.02533) introduce BIM as an extension of FGSM to generate stronger adversaries for higher computational costs. In figure 9 we show the influence of the two hyperparameters $$\alpha$$ and $$num_iter$$ on the attack with the top image from figure 6. We see that BIM is able to generate an adversary which fools the network while FGSM cannot.

{% include image.html file="BIM-Hyperparameter_variation_132.png" description="Effects of the two hyperparameters for BIM. In each row the number of iterations increases from left to right at a constant alpha. On the top left both alpha and number of iterations are 1. This attack is similar to FGSM. Note how the number of iterations has a strong impact on the predicted class whereas alpha does not. We found that increasing alpha further than shown does not have any effect on the confidence."%}

We can see that for this sample increasing alpha generally causes the confidence to drop for smaller epsilons, as shown on the vertical axis from the top left corner. By increasing alpha only it is not possible to change the predicted class. This is expected since alpha is the parameter from the “fast” part of BIM and FGSM was not able to change the class for this image. However, changing the class is possible by increasing the number of iterations. The best results are in the top right plot (lowest alpha, highest number of iterations). For two epsilons the networks predicts a false class with almost 80% confidence. Considering that FGSM was not able to change the class at all this is a strong result. The authors recommend keeping alpha at 1/255 and changing the number of iterations with the heuristic shown in the section Implementation based on epsilon. For the remainder of this report we will choose these hyperparameters according to these recommendations.

In figure 10 we attack an image with high and low clean confidence, as we have done with FGSM in figure 7 and 8. In line with hypothesis 1 it is harder for BIM to manipulate the top image. However, while FGSM is able to change the predicted class for a few epsilons and at low confidence only, BIM is able to achieve an adversarial confidence of almost 80%. Moreover, the “bounce-back” effect seen with FGSM is not present here.


{% include image.html file="BIM_Individual_Images-Dont_Bounce_Back.png" description="Example with high (top) and low (bottom) clean confidence. Clean image is on the left, the adversarial confidence for select epsilon second, third top 5 confidence for the clean case and top 5 confidence for the highest adversarial confidence. In contrast to FGSM BIM is able to generate six adversaries in the top case. For the bottom however BIM is not able to achieve higher confidence than FGSM."%}

Interestingly, for the bottom image BIM is not able to generate higher confidence with the adversary. FGSM is work “sufficiently well” in this case. This leads us to the second hypothesis:

**Hypothesis 2:** For images with low initial confidence, FGSM’s ability to trick the network into predicting a false class is similar to BIM.

Given that in both cases the best adversary is for small epsilon it is likely that for both methods the perturbations are imperceptible. We investigate the perceptibility of adversaries generated by different methods in section 4 below. 

To analyze the performance of BIM further we now look at the general behaviour on all samples.


#### All Images
As for FGSM we generate adversaries for different initial confidence ranges. The left side of figure 11 shows that BIM is able to generate adversaries much more consistently than FGSM. For an epsilon greater 10 almost all attacks are successful. In accordance with hypothesis 1 the slopes for lower confidence ranges are greater as well while being generally greater than for FGSM. In particular, as stated in hypothesis 2 they are very similar to FGSM.

{% include image.html file="BIM-Accuracies_Confidences.png" description="Figure 11: Top 1 accuracy and adversarial confidence for BIM and how they compare to FGSM. Overall BIM is able to generate adversaries more successfully and at higher confidences. Note that as for FGSM if the clean confidence is higher, it is on average harder to generate adversaries for smaller epsilons, which also have a lower confidence."%}

On the right side you can see that BIM generates not only more consistently, but also more confident adversaries. Note the dip for low epsilons. Since we use an alpha of 1 for small epsilons BIM and FGSM are fairly similar. When increasing epsilon the number of iterations increases as well, which makes BIM much more successful as explained above. Note that hypothesis 2 does not claim that for low clean confidences the adversaries generated by BIM and FGSM are similarly confident. Even small epsilons produce much more confident adversaries.

So, is BIM better than FGSM? We found that if the clean confidence is low and the goal is only to change the predicted class, FGSM is preferable since the computational cost is much lower. For more confident adversaries or higher clean confidence however, BIM outperforms FGSM.

The two methods analyzed so far generate untargeted attacks. Next, we investigate the ILLM which targets the least likeliest class.


### 2.3 Least Likeliest Class Method
The ILLM method is related to BIM but is targeted by making the network predict the class with the lowest clean confidence, see section Implementation. Recall from figure 1 how for a well-trained classifier with a softmax output layer there generally is a significant gap between the highest and second highest confidence in the confidence distribution.

While BIM tries to increase the loss on the correct class, which then leads to the second class becoming more likely, ILLM tries to decrease the loss on the class with the lowest confidence (see minus sign in equation 3.2 from section Implementation/ILLM). We found that the result often is a low overall confidence as seen in figure 12. Note that ILLM is on average also less successful in attacking the model than the FGSM.

{% include image.html file="ILLM-compare_attacks_FGSM_BIM.png" description="Figure 12: Top 1 accuracy (left) and adversarial confidences (right) for FGSM, BIM and ILLM. The higher probability that the adversarial class is very different from the correct one comes at the cost of significantly worse performance in comparison to the related BIM method."%}

For future investigation it would be interesting to see how the order of the classes between the first (highest clean confidence) and last (lowest clean confidence) is effected by ILLM.


#### Individual Images
We find examples where ILLM performs poorly and well. In the top image of figure 13 the adversary tricks the network into believing it is looking at a hoop skirt rather than an apron. Considering that there are 1000 possible classes, including for example “baseball”, this choice seems poor since subjectively it is not very different. Additionally, the confidence is low. An adversary generated by BIM for example predicts “Arabian Camel” with 28% confidence.

{% include image.html file="ILLM-confidences_two_examples.png" description="Figure 13: Confidences for two ILLM attacks. In the top example ILLM is able to generate an adversary with low confidence only. The rightmost plot shows how flat the confidence distribution for the lowest successful attack is. In the bottom example the adversary generates the same class as BIM, however at a lower confidence."%}

The confidence distribution in the top image is as expected relatively flat. It appears as if ILLM gives much more confidence to each class rather than concentrating the confidence on a few classes, as explained above. 

In the bottom image of figure 13 you can see another example. With the first epsilon the adversary is able to change the class to “lionfish”, the same as BIM does. However, as you can see on the rightmost plot, the confidence achieves over 80% adversarial confidence into same class. At the same time for ILLM the second class is higher than for BIM. This supports the notion that ILLM “flattens out” the confidence distribution rather than increasing the gap between 1st and 2nd.


++ Figure 14a: Add two outlier examples with high adversarial confidence


In conclusion, we have found that BIM is sufficient for application not only to datasets with few classes but also to datasets with a lot of classes like ImageNet. In particular we found that ILLM does not consistently generate “interesting” examples (apron). Combined with the very low confidence of these adversaries and a similar perceptibility we did not find benefits of ILLM over BIM.


### 2.4 Deep Fool
The methods which we have analyzed so far use the gradient to increase the loss which allows them to approximate the optimal perturbation. DeepFool finds the projected distance to the closest decision boundary and iteratively changes the adversary until the class has changed. As a result the authors claim that the perturbation are significantly less perceptible and adversaries more robust.

Figure 15 shows how confident adversaries generated by DeepFool are in comparison to the other methods. Only if the initial prediction was correct the image is used to generate an adversary. Notably, DeepFool is able to fool the network for all of these images, unlike the other methods. However, the adversaries achieve on a confidence of 30%. A maximum of 50% has been observed. This is significantly lower than BIM for higher epsilon. These numbers don’t change for number of considered classes by DeepFool of 100 and 100.

{% include image.html file="DeepFool-compare_attacks.png" description="Figure 15: Comparison of attacks with DeepFool against other methods. While DeepFool is able to change the class with all attacks successfully, the confidences of the adversaries is on average only around 30%."%}

What influence does the clean confidence have on the adversarial confidence? Figure 16 shows that hypothesis 1 applies. Images with over 99% clean confidence achieve on average 39% adversarial confidence. For lower initial confidences the adversarial confidences are lower accordingly. The minimum observed is 27%.

{% include image.html file="DeepFool-Accuracies_Confidences.png" description="Figure 16: Adversarial confidences for clean confidence ranges. The spread according to hypothesis 1 is smaller for DeepFool than most of BIM. At the same time however the average confidence is lower. The right side shows the general relation between the adversarial confidence and the confidence of the source images."%}

One advantage of DeepFool over the other methods is its robustness. Let’s look at individual images and also inspect perceptibility.


#### Individual Images
The disadvantage of DeepFool seems to be the generally low adversarial confidence. In figure 17 the distribution is shown for the example of the highest adversarial confidence (top). The clean example achieves only about 70% confidence for class “trombone”. The perturbation in the adversary are imperceptible but the network now predicts “violin”. The first and second confidences are almost similar. This makes sense since DeepFool overshoots only slightly the decision boundary. We expect a similar pattern with all images. The first two confidences should be very similar while there’s a gap to the third.

{% include image.html file="DeepFool-two_examples.png" description="Figure 17:  Clean image, adversary, confidences of clean and adversarial images (from left to right). The top shows the example with the highest adversarial confidence while the bottom has the lowest in the data. Note that the perturbation is imperceptible."%}

A similar change in confidence distribution at a lower baseline can be found for the adversary with the lowest confidence. Interestingly this example also has a low clean confidence.



### 3. Perceptibility
Finally, we look at perceptibility of the aforementioned attacks. In figure 18 you can see the unmodified image on the left and attacks with FGSM, BIM and ILLM. To make them comparable we attack with FGSM first and choose the highest adversarial confidence, which is around 24% here. Next, we attack with BIM and find the first adversary for which the model has a similar confidence, also 24%. It achieves this at an epsilon of 4 instead of 20 which makes this attack much less noticeable. For ILLM we cannot find an adversary with similar confidence. Instead, we choose an epsilon of 20 as for FGSM.

{% include image.html file="ILLM-comparison_all_methods.png" description="Figure 18: Attacks with FGSM (second from left), BIM (third) and ILLM (right). While epsilon for FGSM is 20, BIM achieves a similar adversarial confidence of 24% with an epsilon of just 4. Since for ILLM the adversarial confidence is consistently low, we chose epsilon of 20 for the right image."%}

Subjectively, BIM generates the least perceptible adversary. At the same time it also generates the most confident adversary with around 60% for an epsilon of 12. We have seen this for other images as well. BIM was for example able to change a correct, 90% confident detection of a fly to a 100% confident prediction of a bow (sample 894).


## 4. Conclusions
