---

layout: page
title: 4. Conclusions

---

Starting with the simplest adversarial attack, the Fast Gradient Sign Method (FGSM). It is fairly computationally inexpensive attack that worked against most of the sample images. FGSM, however, has difficulties against some of the more difficult images with higher initial confidences.  Also, the manipulations to the image are most of the time noticeable. This attack may have its place in quickly generating large quantities of adversarial examples.

One extension of FGSM, the Basic Iterative Method (BIM), is more computationally expensive. However, it is also much more effective in both attacking the network and hiding the perturbations from humans. In our experiments BIM succeeded almost every of the time with he most confident adversaries.

Adversaries which are truly imperceptible can be generated by DeepFool, a different type of algorithm.

## Further Reading and Other Work
Carlini and Wagner [propose](http://arxiv.org/abs/1608.04644) three strong attacks which outperform all existing attack methods, including FGSM and DeepFool. Their attacks are based on three distance metrics which qualify the requirements of a successful adversary. They ensure that the adversary is strong, imperceptible and achieves a high confidence of the targeted class. 

Their three attacks beat all existing defenses. In particular they suggest to secure a model against their most powerful $$L_{2}$$ attack and show that transferability fails using high confidence adversaries.
