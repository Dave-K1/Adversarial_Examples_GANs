{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "This notebook contains some functions that are used frequently throughout this project. They are explained here. Copies of these functions are available in the `modules/helper.py` so that they can be imported in the different notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For the data we use a subset of the ImageNet [1] dataset. It was part of the competition [NIPS 2017: Adversarial Learning Development Set](https://www.kaggle.com/google-brain/nips-2017-adversarial-learning-development-set#categories.csv) [2] on Kaggle and available [here](http://www.kaggle.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetSubset(Dataset):\n",
    "    '''Imports subset of the ImageNet dataset from the Kaggle competion'''\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "        csv_file (string)              -- Path to the csv file with metadata like labels and fileId.\n",
    "        root_dir (string)              -- Directory with all the images.\n",
    "        transform (callable, optional) -- Optional transform to be applied on a sample.\n",
    "        '''\n",
    "\n",
    "        self.images_meta = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.images_meta)\n",
    "    \n",
    "    def __getitem__(self, idx):     \n",
    "        image_path = self.root_dir\n",
    "        image_name = self.images_meta[\"ImageId\"][idx]\n",
    "        label = self.images_meta[\"TrueLabel\"][idx]\n",
    "        \n",
    "        ## Load image\n",
    "        image = Image.open(image_path + image_name + \".png\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        ## Format label. Labels in dataset are 1 indexed but 0 indexed in model. Make all 0 indexed.\n",
    "        label = torch.tensor(label-1, dtype=torch.long)\n",
    "        \n",
    "        ## Move data to cuda if available\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_name(idx):\n",
    "    '''\n",
    "    Converts the output class index from the googleNet to the respective name.\n",
    "    \n",
    "    Input:\n",
    "    idx  -- Class index as integer\n",
    "    \n",
    "    Returns:\n",
    "    name -- Class names corresponding to idx as string\n",
    "    '''\n",
    "    \n",
    "    ## Load dictionary from file    \n",
    "    names = pd.read_csv(\"./data/ImageNet_subset/categories.csv\")\n",
    "    \n",
    "    ## Retrieve class name for idx\n",
    "    name = names.iloc[idx][\"CategoryName\"]\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_image(tensor):\n",
    "    '''\n",
    "    De-normalizes an image as a tensor and converts it back into an 8bit image object.\n",
    "    \n",
    "    Inputs:\n",
    "    tensor -- PyTorch tensor of shape (1, 3, 224, 224)\n",
    "    \n",
    "    Returns:\n",
    "    image  -- De-normalized image object\n",
    "    '''\n",
    "    \n",
    "    ## Detach computation graph and remove batch dimension\n",
    "    tensor = tensor.detach().clone()    \n",
    "    tensor.squeeze_()\n",
    "    \n",
    "    ## De-normalize tensor image\n",
    "    invert_preprocess = transforms.Compose([\n",
    "        transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "    ])\n",
    "      \n",
    "    image = invert_preprocess(tensor)      \n",
    "    image = np.array(image.detach())\n",
    "    \n",
    "    ## Rescale to range 0-255 and convert datatype into 8bit\n",
    "    image = image * 255    \n",
    "    image = np.uint8(image)\n",
    "    \n",
    "    ## Swap axes to get the expected shape (224, 224, 3)\n",
    "    image = np.swapaxes(image, 0, 2)\n",
    "    \n",
    "    ## Rotate image and convert to image object\n",
    "    for i in range(3):\n",
    "        image = np.rot90(image)\n",
    "    \n",
    "    image  = Image.fromarray(image)\n",
    "    \n",
    "    ## Show image\n",
    "    plt.imshow(image)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "We use the trained googleNet [3]. It is available through the `Torchvision` library. It expects the data to be 3 channel RGB with a size of at least 224. The data has to be scaled into the range $[0, 1]$ and then centered, see [here](https://pytorch.org/hub/pytorch_vision_googlenet/). These steps are implemented in 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image, target_label, return_grad=False):\n",
    "    '''\n",
    "    Predicts the class of the given image and compares the prediction with the provided label.\n",
    "    \n",
    "    Inputs:\n",
    "    model           -- net\n",
    "    image           -- Input image as tensor of shape (1, 3, 224, 224)\n",
    "    target_label    -- Target label as tensor of shape (1)\n",
    "    return_grad     -- Returns gradient if set True\n",
    "    \n",
    "    Returns:\n",
    "    gradient        -- None if return_grad=False. Otherwise the gradient from the prediction \n",
    "                       as a tensor of shape ()\n",
    "    top_1           -- Integer of value 1 if class is correct, otherwise 0\n",
    "    top_5           -- Integer of value 1 if target class is among the 5 most confident predicted classes\n",
    "    confidence      -- Confidence of prediction\n",
    "    predicted_label -- Predicted label as integer\n",
    "    '''      \n",
    "        \n",
    "    if return_grad == True:\n",
    "        image.requires_grad=True\n",
    "        prediction = model(image)\n",
    "               \n",
    "        # Zero gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate loss using the class index for pandas and get gradient\n",
    "        loss = F.nll_loss(prediction, target_label)\n",
    "        loss.backward()\n",
    "        gradient = image.grad.data\n",
    "        \n",
    "    else:           \n",
    "        gradient = None\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image)\n",
    "   \n",
    "\n",
    "    ## Get class index and confidence for prediction \n",
    "    prediction = torch.nn.functional.softmax(prediction[0].cpu().detach(), dim=0).numpy()\n",
    "    \n",
    "    \n",
    "    ## Get class label indices corresponding to the five highest confidences\n",
    "    predicted_class_index = prediction.argsort()[-5:][::-1]\n",
    "\n",
    "        \n",
    "    ## Get largest confidences\n",
    "    confidence = prediction[predicted_class_index[0]]\n",
    "    \n",
    "    \n",
    "    ## Calculate if prediction is correct        \n",
    "    if predicted_class_index[0] == target_label:\n",
    "        top_1 = 1\n",
    "        \n",
    "    else:\n",
    "        top_1 = 0\n",
    "     \n",
    "    \n",
    "    ## Calculate top 5 accuracy\n",
    "    if target_label.numpy() in predicted_class_index:\n",
    "        top_5 = 1\n",
    "    else:\n",
    "        top_5 = 0\n",
    "    \n",
    "    \n",
    "    return gradient, top_1, top_5, confidence, predicted_class_index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(image_clean, image_adv, conf_clean, conf_adv, label_clean, label_adv, label_target):\n",
    "    '''\n",
    "    Plots the clean and adversarial image side-by-side. Prints predicted labels and confidences for both.\n",
    "    \n",
    "    Inputs:\n",
    "    image_clean     -- Clean image as tensor of shape (1, 1, 28, 28)\n",
    "    image_adv       -- Adversarial image as tensor of shape (1, 1, 28, 28)\n",
    "    conf_clean      -- Confidence for the clean image\n",
    "    conf_adv        -- Confidence for the adversarial image\n",
    "    label_clean     -- Predicted label from the clean image\n",
    "    label_adv       -- Predicted label from the adversarial image\n",
    "    label_target    -- Target label as tensor of shape (1)\n",
    "    '''\n",
    "   \n",
    "    ## Get label names from index\n",
    "    name_target = idx_to_name(label_target.detach().numpy()[0])\n",
    "    name_clean = idx_to_name(label_clean)\n",
    "    name_adv = idx_to_name(label_adv)\n",
    "    \n",
    "    ## Text\n",
    "    print(\"\\t\\t\\tClean image\\t Adversarial image\\n\")    \n",
    "    print(\"Actual class: \\t\\t{}\\t\\t\\t{}\".format(name_target, name_target ))\n",
    "    print(\"Predicted class: \\t{}\\t\\t\\t{}\".format(name_clean, name_adv ))\n",
    "    print(\"Confidence: \\t\\t{:.2f}%\\t\\t\\t\\t{:.2f}%\\n\".format(conf_clean*100, conf_adv*100))\n",
    "    \n",
    "    ## Plots\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"Clean example\", fontsize=30)\n",
    "    show_tensor_image(image_clean)\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Adversarial example\", fontsize=30)\n",
    "    show_tensor_image(image_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on clean images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] &emsp; Russakovsky et al. (2015) *ImageNet Large Scale Visual Recognition Challenge*\n",
    "\n",
    "[2] &emsp; Kurakin et al. (2018) *Adversarial Attacks and Defences Competition*\n",
    "\n",
    "[3] &emsp; Szegedy et al. (2015) *Going Deeper with Convolutions*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
