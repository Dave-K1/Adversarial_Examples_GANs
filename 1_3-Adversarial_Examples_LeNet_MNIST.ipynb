{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Examples - LeNet on MNIST Data\n",
    "\n",
    "LeNet style network on the development set of MNIST dataset. 10,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "import urllib\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and model\n",
    "\n",
    "First we load the model and data.\n",
    "\n",
    "\n",
    "### 1.1 Model\n",
    "\n",
    "LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load model and data\n",
    "pretrained_model = \"models/lenet_mnist_model.pth\"\n",
    "\n",
    "## Define LeNet model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "## Initialize the network\n",
    "model = Net()\n",
    "\n",
    "## Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "## Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data\n",
    "\n",
    "Loading the data into a PyTorch dataloader. Here we also integrate the scaling transformation step.\n",
    "\n",
    "We also define the function `show_tensor_image`to display an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ])), \n",
    "        batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset.targets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(data_loader, image_number=\"random\"):\n",
    "    '''\n",
    "    Returns a random image of given label as tensor object\n",
    "    \n",
    "    Inputs:\n",
    "    data_loader  -- PyTorch dataloader with the MNIST data\n",
    "    image_number -- Number of example to show. There are 10,000. Default is random\n",
    "    \n",
    "    Returns:\n",
    "    image        -- Image as tensor of dimension (1, 1, 28, 28)\n",
    "    label        -- Label as integer\n",
    "    '''\n",
    "    \n",
    "    if image_number == \"random\":\n",
    "        i = random.randrange(0, 10000)\n",
    "    else:\n",
    "        i = int(image_number)\n",
    "    \n",
    "    ## Extract data\n",
    "    image = data_loader.dataset.data[i,:,:]\n",
    "\n",
    "    ## Add batch and channel dimension as required by net\n",
    "    image = image.unsqueeze(0).unsqueeze(0)\n",
    "    image = image.float()\n",
    "    \n",
    "    ## Extract label\n",
    "    label = int(data_loader.dataset.targets[i].numpy())\n",
    "       \n",
    "    ## Print image number\n",
    "    print(\"Image number {} chosen\\n\".format(int(i)))\n",
    "    \n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_image(tensor):\n",
    "    '''\n",
    "    Plots the given image as tensor.\n",
    "    \n",
    "    Inputs:\n",
    "    tensor -- image as tensor of dimension (1, 28, 28)\n",
    "    \n",
    "    Returns:\n",
    "    Plot of the image\n",
    "    '''\n",
    "    \n",
    "    ## Remove batch and channel dimension\n",
    "    tensor = tensor.detach()\n",
    "    tensor = tensor.squeeze(0).squeeze(0)\n",
    "\n",
    "    ## Plot\n",
    "    plt.imshow(tensor.numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image, label, return_grad=False, supress_output=True):\n",
    "    '''\n",
    "    Predicts\n",
    "    \n",
    "    Inputs:\n",
    "    model          -- net\n",
    "    image          -- Input image as tensor of shape (1, 1, 28, 28)\n",
    "    label          -- class label as tensor\n",
    "    return_grad    -- Returns gradient if set True\n",
    "    supress_output -- Prints output if set False\n",
    "    \n",
    "    Returns:\n",
    "    gradient       -- None if return_grad=False. Otherwise the gradient from the prediction as a tensor. \n",
    "    accuracy       -- Integer of value 1 if class is panda, otherwise 0\n",
    "    confidence     -- Confidence of prediction\n",
    "    '''\n",
    "        \n",
    "    if return_grad == True:\n",
    "        image.requires_grad=True\n",
    "        prediction = model(image)\n",
    "               \n",
    "        # Zero gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate loss using the class index for pandas and get gradient\n",
    "        loss = F.nll_loss(prediction, torch.tensor([label]))\n",
    "        loss.backward()\n",
    "        gradient = image.grad.data\n",
    "        \n",
    "    else:           \n",
    "        gradient = None\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image)\n",
    "    \n",
    "    ## Calculate if prediction is correct\n",
    "    class_index = np.argmax(prediction.detach().numpy())\n",
    "    \n",
    "    if class_index == label:\n",
    "        accuracy = 1.0\n",
    "        \n",
    "    else:\n",
    "        accuracy = 0.0\n",
    "     \n",
    "    \n",
    "    ## Get class index and confidence\n",
    "    prediction = torch.nn.functional.softmax(prediction[0].detach(), dim=0).numpy()    \n",
    "    confidence = prediction[class_index] * 100\n",
    "    \n",
    "    ## Get class name from the predicted index\n",
    "    name = np.argmax(prediction)\n",
    "   \n",
    "    if supress_output == False:\n",
    "        print(\"Predicted class:\\n{}\\n{:.2f} % confidence\\n\".format(name, confidence))\n",
    "        print(\"Actual class: \\n{}\".format(label))\n",
    "    \n",
    "    return gradient, accuracy, confidence/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Adversarial Examples\n",
    "\n",
    "Functions for generating the examples. The details are discussed in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_fgsm(image, epsilon, grad_x):\n",
    "    '''\n",
    "    Applies FGSM attack on input image.\n",
    "    \n",
    "    Inputs:\n",
    "    image       -- Image data as tensor\n",
    "    epsilon     -- Hyperparameter\n",
    "    grad_x      -- Gradient of the cost with respect to x\n",
    "    \n",
    "    Returns:\n",
    "    image_tilde -- Adversarial image as tensor\n",
    "    '''\n",
    "    \n",
    "    ## Compute eta part\n",
    "    eta = epsilon * grad_x.sign()\n",
    "    \n",
    "    ## Apply perturbation\n",
    "    image_tilde = image + eta\n",
    "    \n",
    "    ## Clip image to maintain the range [0, 1]\n",
    "    image_tilde = torch.clamp(image_tilde, 0, 1)\n",
    "    \n",
    "    return image_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Attacks\n",
    "\n",
    "## 4.1 Fast Gradient Sign Method\n",
    "\n",
    "This method by [1] generates adversarial examples quickly. It computes a step of gradient descent and moves one step of magnitude $\\epsilon$ into the direction of this gradient:\n",
    "\n",
    "$\\widetilde{x} = x + \\eta$\n",
    "\n",
    "$\\eta = \\epsilon \\cdot sign(\\nabla_{x} J(\\Theta, x, y))$\n",
    "\n",
    "This method computes examples quickly where $\\epsilon$ is a hyperparameter. Let's inspect a few clean and perturbed examples for small and large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(image_as_tensor, label, epsilon):\n",
    "    '''\n",
    "    Generates an adversarial image. Plots the clean and adversarial image side-by-side\n",
    "    \n",
    "    Inputs:\n",
    "    image_as_tensor -- Image as tensor\n",
    "    label           -- Class label for target class\n",
    "    epsilon         -- Hyperparameter for FGSM\n",
    "    '''\n",
    "    \n",
    "    ## Predict for clean image\n",
    "    gradient, _, _ = predict(model, image_as_tensor, label=label, return_grad=True, supress_output=False)\n",
    "\n",
    "    print(\"\\n==============================\\nPerturbed image: \\n\")\n",
    "\n",
    "    ## Compute adversarial image and predict for it.\n",
    "    perturbed_data = generate_adversarial_fgsm(image_as_tensor, epsilon, gradient)\n",
    "    predict(model, perturbed_data, label=label, return_grad=False, supress_output=False)\n",
    "\n",
    "\n",
    "    ## Plots\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"Clean example\", fontsize=30)\n",
    "    show_tensor_image(image_as_tensor)\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Adversarial example\", fontsize=30)\n",
    "    show_tensor_image(perturbed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random example at lower epsilon\n",
    "epsilon = 12/255\n",
    "\n",
    "\n",
    "image_as_tensor, label = image_to_tensor(data_loader)\n",
    "\n",
    "\n",
    "plot_example(image_as_tensor, label, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random example at higher epsilon\n",
    "epsilon = 50/255\n",
    "\n",
    "image_as_tensor, label = image_to_tensor(data_loader, image_number=5761)\n",
    "plot_example(image_as_tensor, label, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, for cetrain values of $\\epsilon$ the perturbations are clearly visable to the human eye. At the same time, the exampels are still mostly classified as the correct class, however at a lower confidence.\n",
    "\n",
    "For $\\epsilon = 50/255$ sample number `5761` is an example for a desirable adversarial input. The predicted class is `2` with a confidence of over $96$%. However, the actual class is `9`.\n",
    "\n",
    "Next we generate adversarial inputs with different values of $\\epsilon$ and compute the average accuracy and confidence on all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run on all 10000 examples\n",
    "epsilons = [0, 1/255, 2/255, 4/255, 10/255, 20/255, 30/255, 40/255, 50/255, 60/255, 90/255]\n",
    "\n",
    "accuracy = []\n",
    "confidence = []\n",
    "\n",
    "accuracy_adversarial = []\n",
    "confidence_adversarial = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "   \n",
    "    acc_sub = []\n",
    "    conf_sub = []\n",
    "    acc_sub_adver = []\n",
    "    conf_sub_adver = []\n",
    "    i = 1\n",
    "    \n",
    "    for image_as_tensor, label in data_loader:      \n",
    "        clear_output(wait=True)\n",
    "        print(\"Running for epsilon {:.2f}\".format(epsilon))\n",
    "        print(i)\n",
    "        \n",
    "        ## Predict with clean image\n",
    "        gradient, acc, conf = predict(model, image_as_tensor, label=label, return_grad=True)\n",
    "        acc_sub.append(acc)\n",
    "        conf_sub.append(conf)\n",
    "\n",
    "        ## Predict with adversarial image\n",
    "        perturbed_data = generate_adversarial_fgsm(image_as_tensor, epsilon, gradient)\n",
    "        _, acc, conf = predict(model, perturbed_data, label=label)\n",
    "        acc_sub_adver.append(acc)\n",
    "        conf_sub_adver.append(conf)\n",
    "        i += 1\n",
    "        \n",
    "    ## Add accuracies and confidences\n",
    "    accuracy.append(np.mean(acc_sub))\n",
    "    confidence.append(np.mean(conf_sub))\n",
    "    accuracy_adversarial.append(np.mean(acc_sub_adver))\n",
    "    confidence_adversarial.append(np.mean(conf_sub_adver))\n",
    "\n",
    "\n",
    "## Save results\n",
    "result = pd.DataFrame()\n",
    "e = np.array(epsilons) * 255\n",
    "result[\"Epsilon_255\"] = e\n",
    "result[\"Accuracy\"] = accuracy_adversarial\n",
    "result[\"Confidence\"] = confidence_adversarial\n",
    "result.to_csv(\"results/LeNet_MNIST_FGSM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e, accuracy_adversarial, \"s-\", color=\"navy\", label=\"Accuracy\")\n",
    "plt.plot(e, confidence_adversarial, \"^-\", color=\"orange\", label=\"Confidence\")\n",
    "plt.xlabel(\"Epsilon [*255]\")\n",
    "\n",
    "plt.title(\"Adversarial Examples on MNIST Dev Set\", fontsize=30)\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig(\"Adversarial_Examples_MNIST_FGSM.pdf\")\n",
    "plt.savefig(\"Adversarial_Examples_MNIST_FGSM.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Basic Iterative Method (BIM)\n",
    "\n",
    "In the previous section we found examples which are classified as the wrong class at a high confidence using the FGSM. To the human eye however, these examples can clearly be distinguished from the clean examples. Another problem with FGSM is that ist can be used for untargeted attacks only [2].\n",
    "\n",
    "BIM requires the following function, which clips pixel values of an image $X^\\prime$:\n",
    "\n",
    "**TODO**\n",
    "Fix explanation!!\n",
    "\n",
    "Similar to the fast method we update the pixel values:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{1}\n",
    "\\widetilde{X}_{n+1} = Clip_{X, \\epsilon} \\{ \\widetilde{X}_{n} + \\alpha sign(\\nabla_{X} J(\\widetilde{X}_{n}, Y_{true})) \\}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here, the Clip function is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{2}\n",
    "Clip_{X, \\epsilon} \\{ X^\\prime \\} (x, y, z) = min\\{ 255, X(x, y, z) + \\epsilon, max\\{0, X(x, y, z)-\\epsilon, X^\\prime(x, y, z) \\} \\}\n",
    "\\end{equation}\n",
    "\n",
    "For the implementation we initialize:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{3}\n",
    "\\widetilde{X}_{0} = X\n",
    "\\end{equation}\n",
    "\n",
    "For the implementation we do for the number of iterations `n`:\n",
    "\n",
    "\n",
    "a) Compute $ X^\\prime = \\widetilde{X}_{n} + \\alpha sign(\\nabla_{X} J(\\widetilde{X}_{n}, Y_{true}))$ as used in equation (1)\n",
    "\n",
    "b) Compute $ X(x, y, z) + \\epsilon $ and $ X(x, y, z) - \\epsilon $\n",
    "\n",
    "c) Evaluate equation (2) using steps a) and b)\n",
    "\n",
    "$max\\{0, X(x, y, z)-\\epsilon, X^\\prime(x, y, z) \\}$ (1.2)\n",
    "\n",
    "d) Retrieve updated adversarial image $\\widetilde{X}_{1}$ as given in equation (1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image number 6695 chosen\n",
      "\n",
      "0\n",
      "tensor([[-1492.0259, -1172.3378, -1216.6344, -1427.5031,     0.0000, -1192.5981,\n",
      "         -1109.0784, -1099.5205, -1029.8264,  -745.4501]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b9a08c10831f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## Generate test image of shape (1, 1, 28, 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#X_ones = torch.ones((1, 1, 28, 28), dtype=torch.float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_BIM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-bce5ec33f77c>\u001b[0m in \u001b[0;36mapply_BIM\u001b[0;34m(model, image, label, alpha, epsilon, num_iterations)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m## Compute X_prime according to equation (1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mimage_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_adver\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_adver\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m## Apply clip function to image_prime as in equation (2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Test cases\n",
    "epsilon = 10/255\n",
    "alpha = 1/255\n",
    "n = 5\n",
    "\n",
    "X_test, Y_test = image_to_tensor(data_loader, image_number='random')\n",
    "\n",
    "## Generate test image of shape (1, 1, 28, 28)\n",
    "#X_ones = torch.ones((1, 1, 28, 28), dtype=torch.float)\n",
    "X_adv = apply_BIM(model, X_test, Y_test, alpha, epsilon, num_iterations=n)\n",
    "\n",
    "torch.equal(X_test, X_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_BIM(model, image, label, alpha, epsilon, num_iterations=2):\n",
    "    '''\n",
    "    Applies given number of steps of the Basic Iterative Method (BIM) attack on the input image.\n",
    "    \n",
    "    Inputs:\n",
    "    model          -- Model under attack\n",
    "    image          -- Image data as tensor of shape (1, 1, 28, 28)\n",
    "    label          -- Label from image of shape (1)\n",
    "    alpha          -- Hyperparameter for iterative step??\n",
    "    epsilon        -- Hyperparameter for sign method???\n",
    "    num_iterations -- Number of iterations to perform\n",
    "    \n",
    "    Returns:\n",
    "    image_adver    -- Adversarial image as tensor\n",
    "    '''\n",
    "    \n",
    "    ## Convert label to torch tensor\n",
    "    label = torch.tensor(label, dtype=torch.long).unsqueeze(0)    \n",
    "    \n",
    "    ## Check input image and label shapes\n",
    "    assert(image.shape == torch.Size([1, 1, 28, 28]))\n",
    "    assert(label.shape == torch.Size([1]))\n",
    "    \n",
    "    ## Initialize adversarial image as image according to equation (3)\n",
    "    image_adver = image.clone()    \n",
    "    #image_adver.requires_grad=True\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        print(i)\n",
    "        \n",
    "        ## Make a copy and detach so the computation graph can be constructed\n",
    "        image_adver = image_adver.clone().detach()\n",
    "        image_adver.requires_grad=True\n",
    "        \n",
    "        ## Compute cost with example image_adersarial        \n",
    "        #image_adver.retain_grad=True\n",
    "        \n",
    "        pred = model(image_adver)\n",
    "        \n",
    "        print(pred)\n",
    "        #class_index = np.argmax(pred.detach().numpy())\n",
    "        \n",
    "        loss = F.nll_loss(pred, label)        \n",
    "        model.zero_grad()        \n",
    "        loss.backward()        \n",
    "        grad_x = image_adver.grad.data       \n",
    "        \n",
    "        ## Checks\n",
    "        assert(image_adver.grad is not None)\n",
    "        \n",
    "        \n",
    "        ## Compute X_prime according to equation (1)\n",
    "        image_prime = image_adver + alpha * grad_x.detach().sign()\n",
    "        assert(torch.equal(image_prime, image_adver) == False)\n",
    "    \n",
    "        ## Apply clip function to image_prime as in equation (2)       \n",
    "        image_plus = image + epsilon\n",
    "        image_minus = image - epsilon\n",
    "        assert(torch.equal(image_plus, image) == False)\n",
    "        \n",
    "        # Equation 1.2\n",
    "        third_part_1 = torch.max(image_minus, image_prime)\n",
    "        third_part = torch.max(torch.tensor(0, dtype=torch.float), third_part_1)\n",
    "              \n",
    "        # Equation (2)\n",
    "        image_adver = torch.min(image_plus, third_part)                 \n",
    "        image_adver = torch.min(torch.tensor(255, dtype=torch.float), image_adver)                        \n",
    "\n",
    "    \n",
    "    return image_adver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "Fast gradient sign method produces examples which can be detected by a human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] &emsp; Goodfellow et al. (2015) *Explaining and Harnessing Adversarial Examples*\n",
    "\n",
    "[2] &emsp; Kurakin et al. (2017) *Adversarial examples in the physical world*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
